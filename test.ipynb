{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c724824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from chromadb import PersistentClient\n",
    "import litellm\n",
    "from pydantic import BaseModel, Field\n",
    "from pathlib import Path\n",
    "from tenacity import retry, wait_exponential\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26bf6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-oss:20b\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "DB_NAME = \"preprocessed_db\"\n",
    "collection_name = \"docs\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "KNOWLEDGE_BASE_PATH = Path(\"notes-new\")\n",
    "AVERAGE_CHUNK_SIZE = 500\n",
    "# KNOWLEDGE_BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75f0fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(base_url=OLLAMA_BASE_URL,api_key='ollama')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62e294",
   "metadata": {},
   "source": [
    "## Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81f8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by LangChain's Document - let's have something similar\n",
    "\n",
    "class Result(BaseModel):\n",
    "    page_content: str\n",
    "    metadata: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eefc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to perfectly represent a chunk\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    headline: str = Field(description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\")\n",
    "    summary: str = Field(description=\"A few sentences summarizing the content of this chunk to answer common questions\")\n",
    "    original_text: str = Field(description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\")\n",
    "\n",
    "    def as_result(self, document):\n",
    "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
    "        return Result(page_content=self.headline + \"\\n\\n\" + self.summary + \"\\n\\n\" + self.original_text,metadata=metadata)\n",
    "\n",
    "\n",
    "class Chunks(BaseModel):\n",
    "    chunks: list[Chunk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d735d5af",
   "metadata": {},
   "source": [
    "## Fetch Documents from knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents():\n",
    "    \"\"\"A homemade version of the LangChain DirectoryLoader\"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for folder in KNOWLEDGE_BASE_PATH.iterdir():\n",
    "        doc_type = folder.name\n",
    "        for file in folder.rglob(\"*.md\"):\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                documents.append({\"type\": doc_type, \"source\": file.as_posix(), \"text\": f.read()})\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2131911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 146 documents\n"
     ]
    }
   ],
   "source": [
    "documents = fetch_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e21930",
   "metadata": {},
   "source": [
    "## Use LLM for Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbfbac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(document):\n",
    "    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
    "\n",
    "    return f\"\"\"\n",
    "You are preparing documents for a fully local Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "The source is an Obsidian knowledge vault (Markdown files).\n",
    "\n",
    "This content will be embedded and stored in a vector database for semantic + keyword retrieval.\n",
    "\n",
    "Document metadata:\n",
    "- Vault category: {document[\"type\"]}\n",
    "- File path: {document[\"source\"]}\n",
    "\n",
    "Your task is to split this Markdown document into overlapping, retrieval-optimized chunks.\n",
    "\n",
    "IMPORTANT OBJECTIVES:\n",
    "\n",
    "1. Preserve Markdown structure:\n",
    "   - Headers (#, ##, ###)\n",
    "   - Bullet lists\n",
    "   - Code blocks\n",
    "   - Tables\n",
    "   - Paragraph boundaries\n",
    "\n",
    "2. Respect semantic coherence:\n",
    "   - Do NOT split in the middle of logical ideas.\n",
    "   - Prefer chunk boundaries at section headers.\n",
    "   - Sub-chunk long sections if needed.\n",
    "\n",
    "3. Maintain Obsidian context:\n",
    "   - Preserve wiki links ([[Note Name]])\n",
    "   - Preserve tags (#tag)\n",
    "   - Keep heading hierarchy\n",
    "   - Do NOT remove backlinks or references.\n",
    "\n",
    "4. Chunking rules:\n",
    "   - Target approximately {how_many} chunks (flexible).\n",
    "   - Each chunk should be ~400–800 tokens.\n",
    "   - Include ~20–25% overlap (or ~50–100 words) between adjacent chunks.\n",
    "   - Ensure NO information is lost.\n",
    "\n",
    "5. For each chunk, output:\n",
    "\n",
    "- chunk_id (incrementing integer)\n",
    "- headline (derived from closest Markdown header)\n",
    "- summary (2–3 sentence semantic summary)\n",
    "- headings (full heading path, e.g. H1 > H2 > H3)\n",
    "- tags (if present)\n",
    "- wiki_links (all [[links]] inside this chunk)\n",
    "- text (original chunk content, unmodified Markdown)\n",
    "\n",
    "Together, your chunks MUST reconstruct the entire document with overlap.\n",
    "\n",
    "This content will later be used for:\n",
    "\n",
    "- Dense embedding retrieval\n",
    "- BM25 keyword search\n",
    "- Graph-based retrieval via backlinks\n",
    "- Context assembly for a local LLM\n",
    "\n",
    "Do NOT hallucinate.\n",
    "Do NOT rewrite content.\n",
    "Do NOT drop formatting.\n",
    "\n",
    "Here is the document:\n",
    "\n",
    "{document[\"text\"]}\n",
    "\n",
    "Respond ONLY with a structured list of chunks following the schema above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a6688c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_messages(document):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": make_prompt(document)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "def process_document(document):\n",
    "    messages = make_messages(document)\n",
    "    response = completion(\n",
    "        model=\"ollama/MODEL_NAME\",  # prefix with \"ollama/\"\n",
    "        messages=messages,\n",
    "        response_format=Chunks\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
    "    return [chunk.as_result(document) for chunk in doc_as_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b29ab95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "def process_document(document):\n",
    "    messages = make_messages(document)\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"qwen2.5:14b\",\n",
    "        messages=messages,\n",
    "        format=Chunks.model_json_schema(),\n",
    "        options={\n",
    "            \"num_predict\": 8192,  # Max tokens to generate\n",
    "            \"temperature\": 0.1,   # Lower temperature for more consistent JSON\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    reply = response['message']['content']\n",
    "    \n",
    "    if not reply:\n",
    "        raise ValueError(\"Model returned empty response\")\n",
    "    \n",
    "    print(\"Reply length:\", len(reply))  # Debug\n",
    "    \n",
    "    doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
    "    return [chunk.as_result(document) for chunk in doc_as_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0892f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(documents):\n",
    "    chunks = []\n",
    "    for doc in tqdm(documents):\n",
    "        chunks.extend(process_document(doc))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08d3c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/146 [01:59<4:47:48, 119.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 6735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/146 [03:10<3:38:03, 90.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 4866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/146 [04:02<2:54:45, 73.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 3919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/146 [06:35<4:08:04, 104.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 6588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/146 [07:15<3:10:52, 81.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 2770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/146 [08:36<3:09:58, 81.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 5585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 7/146 [09:30<2:47:14, 72.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 4071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 8/146 [11:11<3:07:22, 81.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 7414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9/146 [11:16<2:11:38, 57.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9/146 [19:02<4:49:49, 126.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reply length: 30321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Chunks\n  Invalid JSON: EOF while parsing a string at line 54 column 1829 [type=json_invalid, input_value='{\\n  \"chunks\": [\\n    {\\...retriever = vectorstore', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m, in \u001b[0;36mcreate_chunks\u001b[0;34m(documents)\u001b[0m\n\u001b[1;32m      2\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(documents):\n\u001b[0;32m----> 4\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mextend(\u001b[43mprocess_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n",
      "Cell \u001b[0;32mIn[39], line 24\u001b[0m, in \u001b[0;36mprocess_document\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel returned empty response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReply length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(reply))  \u001b[38;5;66;03m# Debug\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m doc_as_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mChunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchunks\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [chunk\u001b[38;5;241m.\u001b[39mas_result(document) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m doc_as_chunks]\n",
      "File \u001b[0;32m~/Code/Obsidian_RAG/.venv/lib/python3.10/site-packages/pydantic/main.py:766\u001b[0m, in \u001b[0;36mBaseModel.model_validate_json\u001b[0;34m(cls, json_data, strict, extra, context, by_alias, by_name)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m    762\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    763\u001b[0m         code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidate-by-alias-and-name-false\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    764\u001b[0m     )\n\u001b[0;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_name\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Chunks\n  Invalid JSON: EOF while parsing a string at line 54 column 1829 [type=json_invalid, input_value='{\\n  \"chunks\": [\\n    {\\...retriever = vectorstore', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
     ]
    }
   ],
   "source": [
    "chunks = create_chunks(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ff01700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baea3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/146 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [32:37<00:00, 13.40s/it] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class ChunkBoundary:\n",
    "    \"\"\"Represents a potential chunk boundary with context\"\"\"\n",
    "    position: int\n",
    "    header_level: int\n",
    "    header_text: str\n",
    "    priority: int  # Higher = better boundary\n",
    "\n",
    "def extract_markdown_structure(text: str) -> List[ChunkBoundary]:\n",
    "    \"\"\"Find all potential chunk boundaries (headers, paragraph breaks)\"\"\"\n",
    "    boundaries = []\n",
    "    \n",
    "    # Find all headers\n",
    "    for match in re.finditer(r'^(#{1,6})\\s+(.+)$', text, re.MULTILINE):\n",
    "        level = len(match.group(1))\n",
    "        boundaries.append(ChunkBoundary(\n",
    "            position=match.start(),\n",
    "            header_level=level,\n",
    "            header_text=match.group(2).strip(),\n",
    "            priority=10 - level  # H1 = priority 9, H2 = 8, etc.\n",
    "        ))\n",
    "    \n",
    "    # Find paragraph breaks (double newlines)\n",
    "    for match in re.finditer(r'\\n\\n+', text):\n",
    "        boundaries.append(ChunkBoundary(\n",
    "            position=match.end(),\n",
    "            header_level=99,  # Not a header\n",
    "            header_text=\"\",\n",
    "            priority=1\n",
    "        ))\n",
    "    \n",
    "    return sorted(boundaries, key=lambda b: b.position)\n",
    "\n",
    "def find_best_boundary(text: str, target_pos: int, boundaries: List[ChunkBoundary], \n",
    "                       min_pos: int, max_pos: int) -> int:\n",
    "    \"\"\"Find the best chunk boundary near target_pos\"\"\"\n",
    "    candidates = [b for b in boundaries if min_pos <= b.position <= max_pos]\n",
    "    \n",
    "    if not candidates:\n",
    "        # Fallback: find nearest sentence boundary\n",
    "        for offset in range(0, max_pos - target_pos + 1, 50):\n",
    "            for punct in ['. ', '.\\n', '! ', '\\n']:\n",
    "                pos = text.find(punct, target_pos + offset)\n",
    "                if min_pos <= pos <= max_pos:\n",
    "                    return pos + len(punct)\n",
    "        return min(max_pos, len(text))\n",
    "    \n",
    "    # Prefer boundaries closer to target with higher priority\n",
    "    best = max(candidates, key=lambda b: (\n",
    "        b.priority * 100 - abs(b.position - target_pos)\n",
    "    ))\n",
    "    return best.position\n",
    "\n",
    "def split_text_into_chunks(text: str, target_chunk_size: int = 600, \n",
    "                          overlap_size: int = 100) -> List[Dict]:\n",
    "    \"\"\"Split text into overlapping chunks using smart boundaries\"\"\"\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    \n",
    "    boundaries = extract_markdown_structure(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_id = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Calculate target end position\n",
    "        target_end = start + target_chunk_size\n",
    "        \n",
    "        if target_end >= len(text):\n",
    "            # Last chunk - take everything\n",
    "            chunks.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'text': text[start:],\n",
    "                'start_pos': start,\n",
    "                'end_pos': len(text)\n",
    "            })\n",
    "            break\n",
    "        \n",
    "        # Find best boundary within acceptable range\n",
    "        min_end = start + int(target_chunk_size * 0.7)  # At least 70% of target\n",
    "        max_end = start + int(target_chunk_size * 1.3)  # At most 130% of target\n",
    "        \n",
    "        actual_end = find_best_boundary(text, target_end, boundaries, \n",
    "                                       min_end, min(max_end, len(text)))\n",
    "        \n",
    "        chunks.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'text': text[start:actual_end],\n",
    "            'start_pos': start,\n",
    "            'end_pos': actual_end\n",
    "        })\n",
    "        \n",
    "        # Next chunk starts with overlap\n",
    "        start = max(start + 1, actual_end - overlap_size)\n",
    "        chunk_id += 1\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_chunk_context(chunk_text: str, full_text: str, start_pos: int) -> Dict:\n",
    "    \"\"\"Extract structural context for a chunk\"\"\"\n",
    "    # Find the heading hierarchy for this chunk\n",
    "    text_before = full_text[:start_pos]\n",
    "    headings = []\n",
    "    \n",
    "    for match in re.finditer(r'^(#{1,6})\\s+(.+)$', text_before, re.MULTILINE):\n",
    "        level = len(match.group(1))\n",
    "        text = match.group(2).strip()\n",
    "        \n",
    "        # Keep only headings that are ancestors of current position\n",
    "        headings = [h for h in headings if h['level'] < level]\n",
    "        headings.append({'level': level, 'text': text})\n",
    "    \n",
    "    # Find closest header in current chunk\n",
    "    chunk_headers = list(re.finditer(r'^(#{1,6})\\s+(.+)$', chunk_text, re.MULTILINE))\n",
    "    headline = chunk_headers[0].group(2).strip() if chunk_headers else (\n",
    "        headings[-1]['text'] if headings else \"Untitled\"\n",
    "    )\n",
    "    \n",
    "    # Extract tags\n",
    "    tags = list(set(re.findall(r'#[\\w\\-]+', chunk_text)))\n",
    "    \n",
    "    # Extract wiki links\n",
    "    wiki_links = list(set(re.findall(r'\\[\\[([^\\]]+)\\]\\]', chunk_text)))\n",
    "    \n",
    "    return {\n",
    "        'headline': headline,\n",
    "        'headings': ' > '.join(h['text'] for h in headings),\n",
    "        'tags': tags,\n",
    "        'wiki_links': wiki_links\n",
    "    }\n",
    "\n",
    "def make_metadata_prompt(chunk: Dict, document: Dict) -> str:\n",
    "    \"\"\"Create a focused prompt for metadata generation only\"\"\"\n",
    "    return f\"\"\"Generate a concise 2-3 sentence semantic summary for this chunk.\n",
    "\n",
    "Document context:\n",
    "- Vault category: {document[\"type\"]}\n",
    "- File path: {document[\"source\"]}\n",
    "- Heading context: {chunk['headings']}\n",
    "- Headline: {chunk['headline']}\n",
    "\n",
    "Chunk content:\n",
    "{chunk['text'][:1000]}{'...' if len(chunk['text']) > 1000 else ''}\n",
    "\n",
    "Respond with ONLY a JSON object:\n",
    "{{\"summary\": \"your 2-3 sentence summary here\"}}\"\"\"\n",
    "\n",
    "def generate_chunk_metadata(chunk: Dict, document: Dict) -> str:\n",
    "    \"\"\"Use LLM to generate only the summary\"\"\"\n",
    "    import ollama\n",
    "    import json\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"qwen2.5:14b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": make_metadata_prompt(chunk, document)}],\n",
    "        format={\"type\": \"object\", \"properties\": {\"summary\": {\"type\": \"string\"}}},\n",
    "        options={\"temperature\": 0.3, \"num_predict\": 256}\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['message']['content'])\n",
    "    return result.get('summary', '')\n",
    "\n",
    "def process_document(document: Dict) -> List[Dict]:\n",
    "    \"\"\"Process a document into enriched chunks\"\"\"\n",
    "    text = document[\"text\"]\n",
    "    \n",
    "    # Step 1: Split text using Python (fast, reliable)\n",
    "    raw_chunks = split_text_into_chunks(text, target_chunk_size=600, overlap_size=100)\n",
    "    \n",
    "    # Step 2: Extract structural metadata (fast, deterministic)\n",
    "    for chunk in raw_chunks:\n",
    "        context = extract_chunk_context(chunk['text'], text, chunk['start_pos'])\n",
    "        chunk.update(context)\n",
    "    \n",
    "    # Step 3: Generate semantic summaries using LLM (only when needed)\n",
    "    enriched_chunks = []\n",
    "    for chunk in raw_chunks:\n",
    "        summary = generate_chunk_metadata(chunk, document)\n",
    "        \n",
    "        enriched_chunks.append({\n",
    "            'chunk_id': chunk['chunk_id'],\n",
    "            'headline': chunk['headline'],\n",
    "            'summary': summary,\n",
    "            'headings': chunk['headings'],\n",
    "            'tags': chunk['tags'],\n",
    "            'wiki_links': chunk['wiki_links'],\n",
    "            'text': chunk['text'],\n",
    "            'source': document['source'],\n",
    "            'type': document['type']\n",
    "        })\n",
    "    \n",
    "    return enriched_chunks\n",
    "\n",
    "def create_chunks(documents: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Process all documents\"\"\"\n",
    "    from tqdm import tqdm\n",
    "    chunks = []\n",
    "    for doc in tqdm(documents):\n",
    "        chunks.extend(process_document(doc))\n",
    "    return chunks\n",
    "\n",
    "# Usage\n",
    "chunks = create_chunks(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59209273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a7214",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "328e4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "def create_embeddings(chunks):\n",
    "    chroma = PersistentClient(path=DB_NAME)\n",
    "    if collection_name in [c.name for c in chroma.list_collections()]:\n",
    "        chroma.delete_collection(collection_name)\n",
    "    \n",
    "    # Extract text from dict chunks\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    # Extract and clean metadata - convert lists to JSON strings\n",
    "    metas = []\n",
    "    for chunk in chunks:\n",
    "        meta = {}\n",
    "        for k, v in chunk.items():\n",
    "            if k == 'text':\n",
    "                continue\n",
    "            # Convert lists to JSON strings\n",
    "            if isinstance(v, list):\n",
    "                meta[k] = json.dumps(v) if v else \"\"\n",
    "            else:\n",
    "                meta[k] = v\n",
    "        metas.append(meta)\n",
    "    \n",
    "    # Load the all-mpnet-base-v2 model (downloads on first use)\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    \n",
    "    # Generate embeddings\n",
    "    vectors = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    collection = chroma.get_or_create_collection(collection_name)\n",
    "    \n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "    \n",
    "    collection.add(ids=ids, embeddings=vectors.tolist(), documents=texts, metadatas=metas)\n",
    "    print(f\"Vectorstore created with {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6c1e84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1155.93it/s, Materializing param=pooler.dense.weight]                        \n",
      "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Batches: 100%|██████████| 18/18 [00:04<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 574 documents\n"
     ]
    }
   ],
   "source": [
    "create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd035bb1",
   "metadata": {},
   "source": [
    "## Visualizing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2edb1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = PersistentClient(path=DB_NAME)\n",
    "collection = chroma.get_or_create_collection(collection_name)\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['type'] for metadata in metadatas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "78109416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for your note types\n",
    "note_types = ['CMM', 'RNN', 'RAG']\n",
    "color_map = {\n",
    "    'CMM': 'blue',\n",
    "    'RNN': 'green', \n",
    "    'RAG': 'red'\n",
    "}\n",
    "\n",
    "# Get colors based on doc_types\n",
    "colors = [color_map.get(t, 'orange') for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "deadcf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='y',\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "# Save to HTML file and open in browser\n",
    "fig.write_html('vector_visualization.html')\n",
    "# Or use this to open automatically\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be64a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=10, b=10, l=10, t=40)\n",
    ")\n",
    "# Save to HTML file and open in browser\n",
    "fig.write_html('vector_visualization.html')\n",
    "# Or use this to open automatically\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efcb776",
   "metadata": {},
   "source": [
    "## Reranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "787cd2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankOrder(BaseModel):\n",
    "    order: list[int] = Field(\n",
    "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ea7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(question, chunks):\n",
    "    MODEL = \"ollama/llama3.2\"  # or any model you have in Ollama\n",
    "# Make sure Ollama is running: ollama serve\n",
    "    system_prompt = \"\"\"\n",
    "You are a document re-ranker.\n",
    "You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
    "The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance, but you may be able to improve on that.\n",
    "You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
    "Reply only with the list of ranked chunk ids, nothing else. Include all the chunk ids you are provided with, reranked.\n",
    "\"\"\"\n",
    "    user_prompt = f\"The user has asked the following question:\\n\\n{question}\\n\\nOrder all the chunks of text by relevance to the question, from most relevant to least relevant. Include all the chunk ids you are provided with, reranked.\\n\\n\"\n",
    "    user_prompt += \"Here are the chunks:\\n\\n\"\n",
    "    \n",
    "    for index, chunk in enumerate(chunks):\n",
    "        # Handle both dict and Result object formats\n",
    "        if isinstance(chunk, dict):\n",
    "            content = chunk.get('page_content') or chunk.get('text', '')\n",
    "        else:\n",
    "            content = chunk.page_content\n",
    "        \n",
    "        user_prompt += f\"# CHUNK ID: {index + 1}:\\n\\n{content}\\n\\n\"\n",
    "    \n",
    "    user_prompt += \"Reply only with the list of ranked chunk ids, nothing else.\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    response = completion(model=MODEL, messages=messages, response_format=RankOrder)\n",
    "    reply = response.choices[0].message.content\n",
    "    order = RankOrder.model_validate_json(reply).order\n",
    "    print(order)\n",
    "    \n",
    "    return [chunks[i - 1] for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9a55e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1194.69it/s, Materializing param=pooler.dense.weight]                        \n",
      "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "RETRIEVAL_K = 10\n",
    "\n",
    "def fetch_context_unranked(question):\n",
    "    # Generate embedding using local model\n",
    "    query = model.encode(question).tolist()\n",
    "    \n",
    "    results = collection.query(query_embeddings=[query], n_results=RETRIEVAL_K)\n",
    "    \n",
    "    chunks = []\n",
    "    for result in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        chunks.append(Result(page_content=result[0], metadata=result[1]))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68eb4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Sequence Model\"\n",
    "chunks = fetch_context_unranked(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c210f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "tags:\n",
      "  - u...\n",
      " $T_y$: output ...\n",
      "---\n",
      "tags:\n",
      "  - m...\n",
      "---\n",
      "tags:\n",
      "  - m...\n",
      "N\n",
      "* Vanilla RNN...\n",
      "---\n",
      "created: 20...\n",
      "---\n",
      "tags:\n",
      "  - e...\n",
      "---\n",
      "tags:\n",
      "  - m...\n",
      "---\n",
      "tags:\n",
      "  - m...\n",
      "; if that seque...\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "94214270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:58:21 - LiteLLM:WARNING\u001b[0m: utils.py:760 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gpt-oss:20b\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reranked \u001b[38;5;241m=\u001b[39m \u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[70], line 28\u001b[0m, in \u001b[0;36mrerank\u001b[0;34m(question, chunks)\u001b[0m\n\u001b[1;32m     21\u001b[0m user_prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReply only with the list of ranked chunk ids, nothing else.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m     25\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt},\n\u001b[1;32m     26\u001b[0m ]\n\u001b[0;32m---> 28\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRankOrder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m reply \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     30\u001b[0m order \u001b[38;5;241m=\u001b[39m RankOrder\u001b[38;5;241m.\u001b[39mmodel_validate_json(reply)\u001b[38;5;241m.\u001b[39morder\n",
      "File \u001b[0;32m~/Code/Obsidian_RAG/.venv/lib/python3.10/site-packages/litellm/utils.py:1748\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1745\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1746\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1747\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1748\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Code/Obsidian_RAG/.venv/lib/python3.10/site-packages/litellm/utils.py:1569\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1569\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1570\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1572\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1573\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1574\u001b[0m ):\n",
      "File \u001b[0;32m~/Code/Obsidian_RAG/.venv/lib/python3.10/site-packages/litellm/main.py:4305\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   4302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   4303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   4304\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 4305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m   4306\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   4307\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   4308\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   4309\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4310\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   4311\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/Obsidian_RAG/.venv/lib/python3.10/site-packages/litellm/main.py:1306\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     model \u001b[38;5;241m=\u001b[39m deployment_id\n\u001b[1;32m   1305\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1306\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_allow_input_examples(\n\u001b[1;32m   1314\u001b[0m     custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider, model\u001b[38;5;241m=\u001b[39mmodel\n\u001b[1;32m   1315\u001b[0m ):\n\u001b[1;32m   1316\u001b[0m     tools \u001b[38;5;241m=\u001b[39m _drop_input_examples_from_tools(tools\u001b[38;5;241m=\u001b[39mtools)\n",
      "File \u001b[0;32m~/Code/Obsidian_RAG/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:470\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[0;32m--> 470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    473\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m         )\n",
      "File \u001b[0;32m~/Code/Obsidian_RAG/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:451\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    449\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Pass model as E.g. For \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m inference endpoints pass in `completion(model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/starcoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    452\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    453\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    454\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    455\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(api_base)\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gpt-oss:20b\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f822cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Obsidian_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
